{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yezhengshao/School/ECE1786/final_project/GOTalk/train'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment to run on colab GPU\n",
    "\n",
    "\n",
    "# gpu_info = !nvidia-smi\n",
    "# gpu_info = '\\n'.join(gpu_info)\n",
    "# if gpu_info.find('failed') >= 0:\n",
    "#   print('Not connected to a GPU')\n",
    "# else:\n",
    "#   print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment to run on colab GPU\n",
    "\n",
    "# from psutil import virtual_memory\n",
    "# ram_gb = virtual_memory().total / 1e9\n",
    "# print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "# if ram_gb < 20:\n",
    "#   print('Not using a high-RAM runtime')\n",
    "# else:\n",
    "#   print('You are using a high-RAM runtime!')\n",
    "\n",
    "# ! pip install datasets transformers\n",
    "# !apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365e10a59f3c445d9fe74b6ef49ab10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# login to huggingface\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"../data/got1_fined.txt\", \"r\")\n",
    "full_texts =[]\n",
    "texts = []\n",
    "labels = []\n",
    "for i in range(905):\n",
    "    line = file.readline()\n",
    "    words = word_tokenize(line)\n",
    "    sentence = TreebankWordDetokenizer().detokenize(words)\n",
    "    if len(sentence) == 0: \n",
    "        print(line, i)\n",
    "    if sentence[-1] == ']':\n",
    "        sentence = sentence + ' '\n",
    "    sentence = sentence.replace(\" ‚Äô \", \" ' \")\n",
    "    full_texts.append(sentence)\n",
    "    # texts.append(TreebankWordDetokenizer().detokenize(words[:-1]))\n",
    "    # labels.append(TreebankWordDetokenizer().detokenize(words[1:]))\n",
    "    \n",
    "train, test = train_test_split(full_texts, test_size=0.2, shuffle=False)\n",
    "\n",
    "\n",
    "file = open('../data/got_train.txt', 'w')\n",
    "for i in train:\n",
    "    file.write(i)\n",
    "file.close()\n",
    "\n",
    "file = open('../data/got_test.txt', 'w')\n",
    "for i in test:\n",
    "    file.write(i)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yezhengshao/opt/anaconda3/envs/ECE1786/lib/python3.7/site-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (80374 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HScomcom/gpt2-game-of-thrones\", truncation=True)\n",
    "\n",
    "SPECIAL_TOKENS = {\n",
    "    \"bos_token\": \"[BOS]\",\n",
    "    \"eos_token\": \"[EOS]\",\n",
    "    \"pad_token\": \"[PAD]\"\n",
    "    # \"additional_special_tokens\": \n",
    "}\n",
    "tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "\n",
    "# train_path = '../data/got_train.txt'\n",
    "train_path = '../data/got3_fined.txt'\n",
    "# test_path = '../data/got_test.txt'\n",
    "\n",
    "#def load_dataset(train_path,test_path,tokenizer):\n",
    "def load_dataset(train_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=100,\n",
    "          )\n",
    "\n",
    "    # test_dataset = TextDataset(\n",
    "    #       tokenizer=tokenizer,\n",
    "    #       file_path=test_path,\n",
    "    #       block_size=100,\n",
    "    #       )   \n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    # return train_dataset,test_dataset,data_collator\n",
    "    return train_dataset,data_collator\n",
    "\n",
    "# train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)\n",
    "train_dataset,data_collator = load_dataset(train_path,tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  780,   339,   564,   247,   264,  2330,   764,   383,  1854,   389,\n",
       "          477,  3223,    11, 13791,   393,  2042,   764,   220, 50258,  1318])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' s too ‚Ä¶ it ‚Äô s cold. [EOS] Yes. Cold and hard and mean, that ‚Äô s the Wall, and the men who walk it. Not like the stories your wet nurse told you. Well, piss on the stories and piss on your wet nurse. This is the way it is, and you ‚Äô re here for life, same as the rest of us.Jon repeated bitterly. The armorer could talk about life. He ‚Äô d had one. He ÔøΩ'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_dataset[99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='HScomcom/gpt2-game-of-thrones', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[BOS]', 'eos_token': '[EOS]', 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '[PAD]'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"HScomcom/gpt2-game-of-thrones\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    f\"GPT2-GOT2-finetuned\",\n",
    "    # output_dir=\"/content\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=3, # number of training epochs\n",
    "    per_device_train_batch_size=1, # batch size for training\n",
    "    # per_device_eval_batch_size=4,  # batch size for evaluation\n",
    "    # eval_steps = 1000, # Number of update steps between two evaluations.\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    save_steps=1000, # after # steps model is saved \n",
    "    warmup_steps=250,# number of warmup steps for learning rate scheduler\n",
    "    push_to_hub=True # push to the huggingface\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset=test_dataset,\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "# trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "text = \"Jon said [BOS]\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt').input_ids\n",
    "# input_ids = encoded_input.input_ids\n",
    "\n",
    "outputs = model.generate(encoded_input.to(device), do_sample=True, max_length=200, temperature=1.1, top_p = 0.8, pad_token_id = 50256)\n",
    "res = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# text = \"Jon grinned and reached under the table to ruffle the shaggy white fur. Inside, one of the guards looked at him and said\"\n",
    "text = 'Ygritte:\"You know nothing, Jon Snow.\" Jon said'\n",
    "text = \"Ben clapped Jon on the shoulder. ‚ÄúCome, let‚Äôs take a walk in the garden. I‚Äôm sure you have much to tell me.‚Äù Jon said\"\n",
    "# text = 'Jon grinned and reached under the table to ruffle the shaggy white fur. Inside, one of the guards looked at him and said'\n",
    "encoded_input = tokenizer(text, return_tensors='pt').input_ids\n",
    "\n",
    "# beam search with force_words for generating dialogue options\n",
    "force_words = ['[BOS]', '[EOS]']\n",
    "force_words_ids = tokenizer(force_words, add_special_tokens=False).input_ids\n",
    "outputs = model.generate(encoded_input.to(device), do_sample=False, max_length=100, temperature=1.6, top_p = 0.9, pad_token_id = 50256, num_beams=10,\n",
    "                         repetition_penalty = 1, force_words_ids=force_words_ids,)\n",
    "\n",
    "\n",
    "# for generating context \n",
    "# outputs = model.generate(encoded_input.to(device), do_sample=True, max_length=100, temperature=1, top_p = 1, pad_token_id = 50256,\n",
    "#                          repetition_penalty = 1.1)\n",
    "\n",
    "\n",
    "res = tokenizer.batch_decode(outputs, skip_speical_tokens=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to huggingface\n",
    "model.push_to_hub(\"GPT2-GOT4-finetuned\")\n",
    "tokenizer.push_to_hub(\"GPT2-GOT4-finetuned\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d128fe176aead1b5f2c446f268fcdf10cba0d3cb275513d8767b97bd302f321f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
