{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noted that we run the following on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory\n",
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intsall packages for colab\n",
    "! pip install datasets transformers\n",
    "! apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "import os\n",
    "print(transformers.__version__)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HScomcom/gpt2-game-of-thrones\", truncation=True, max_length = 512)\n",
    "\n",
    "SPECIAL_TOKENS = {\n",
    "    \"bos_token\": \"[BOS]\",\n",
    "    \"eos_token\": \"[EOS]\",\n",
    "}\n",
    "tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "\n",
    "train_path = 'GOT_Train_Final1.txt'\n",
    "test_path = 'GOT_Test_Final1.txt'\n",
    "\n",
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=50,\n",
    "          )\n",
    "\n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=50,\n",
    "          )   \n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "\n",
    "    return train_dataset,test_dataset,data_collator\n",
    "\n",
    "\n",
    "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"HScomcom/gpt2-game-of-thrones\")\n",
    "# model = AutoModelWithLMHead.from_pretrained(\"gpt2-xl\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    f\"GPT2-large-GOTfinetuned_v5\",\n",
    "    # output_dir=\"/content\", #The output directory\n",
    "    overwrite_output_dir = True, #overwrite the content of the output directory\n",
    "    num_train_epochs = 3, # number of training epochs\n",
    "    per_device_train_batch_size = 2, # batch size for training\n",
    "    per_device_eval_batch_size = 2,  # batch size for evaluation\n",
    "    eval_steps = 50, # Number of update steps between two evaluations.\n",
    "    evaluation_strategy = \"steps\",\n",
    "    save_steps = 300, # after # steps model is saved \n",
    "    # warmup_steps = 50, # number of warmup steps for learning rate scheduler\n",
    "    learning_rate = 9e-6,\n",
    "    logging_strategy = 'steps',\n",
    "    logging_steps = 50,\n",
    "    push_to_hub=True # push to the huggingface\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "# ***** Running training *****\n",
    "#   Num examples = 378\n",
    "#   Num Epochs = 3\n",
    "#   Instantaneous batch size per device = 2\n",
    "#   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
    "#   Gradient Accumulation steps = 1\n",
    "#   Total optimization steps = 567\n",
    "#   Number of trainable parameters = 774032640\n",
    "#  [567/567 11:23, Epoch 3/3]\n",
    "# Step\tTraining Loss\tValidation Loss\n",
    "# 50\t7.001300\t5.906636\n",
    "# 100\t5.125400\t4.208858\n",
    "# 150\t3.968200\t3.765974\n",
    "# 200\t3.614600\t3.627266\n",
    "# 250\t3.252600\t3.604956\n",
    "# 300\t3.242000\t3.568900\n",
    "# 350\t3.140900\t3.545990\n",
    "# 400\t3.062200\t3.538033\n",
    "# 450\t2.877700\t3.557756\n",
    "# 500\t2.877600\t3.549932\n",
    "# 550\t2.848400\t3.549331\n",
    "# ***** Running Evaluation *****\n",
    "#   Num examples = 98\n",
    "#   Batch size = 2\n",
    "# ***** Running Evaluation *****\n",
    "#   Num examples = 98\n",
    "#   Batch size = 2\n",
    "# ***** Running Evaluation *****\n",
    "#   Num examples = 98\n",
    "#   Batch size = 2\n",
    "# ***** Running Evaluation *****\n",
    "#   Num examples = 98\n",
    "#   Batch size = 2\n",
    "# ***** Running Evaluation *****\n",
    "#   Num examples = 98\n",
    "#   Batch size = 2\n",
    "# ***** Running Evaluation *****\n",
    "#   Num examples = 98\n",
    "#   Batch size = 2\n",
    "# Saving model checkpoint to GPT2-large-GOTfinetuned_v5/checkpoint-300\n",
    "# Configuration saved in GPT2-large-GOTfinetuned_v5/checkpoint-300/config.json\n",
    "# Model weights saved in GPT2-large-GOTfinetuned_v5/checkpoint-300/pytorch_model.bin\n",
    "# Several commits (3) will be pushed upstream.\n",
    "# WARNING:huggingface_hub.repository:Several commits (3) will be pushed upstream.\n",
    "# ***** Running Evaluation *****\n",
    "#   Num examples = 98\n",
    "#   Batch size = 2\n",
    "# ***** Running Evaluation *****\n",
    "#   Num examples = 98\n",
    "#   Batch size = 2\n",
    "# ***** Running Evaluation *****\n",
    "#   Num examples = 98\n",
    "#   Batch size = 2\n",
    "# ***** Running Evaluation *****\n",
    "#   Num examples = 98\n",
    "#   Batch size = 2\n",
    "# ***** Running Evaluation *****\n",
    "#   Num examples = 98\n",
    "#   Batch size = 2\n",
    "\n",
    "\n",
    "# Training completed. Do not forget to share your model on huggingface.co/models =)\n",
    "\n",
    "\n",
    "# TrainOutput(global_step=567, training_loss=3.7041593884664867, metrics={'train_runtime': 685.3742, 'train_samples_per_second': 1.655, 'train_steps_per_second': 0.827, 'total_flos': 240994414080000.0, 'train_loss': 3.7041593884664867, 'epoch': 3.0})"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
