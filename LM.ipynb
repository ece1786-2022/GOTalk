{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 Top-Level Code/Notebook\n",
    "### Training a language model base on Karpathy's minGPT codebase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nimport nltk\\nnltk.download('punkt')\\n\""
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code below is needed for using Google Colab, so un comment this if that is what you're using\n",
    "\"\"\" \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\\n%cd /content/drive/MyDrive/Colab\\\\ Notebooks/\\n\""
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code below is also needed for using Google Colab\n",
    "# BEFORE executing this, you must place the mingpt folder supplied in the assignment\n",
    "# your google drive, within the folder \"Colab Notebooks\"\n",
    "#\n",
    "# It mounts and changes into the folder that contains mingpt, which you must upload to google drive\n",
    "# So un-comment it if you've uploaded mingpt to your google drive, into the  \"Colab Notebooks\" folder\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/Colab\\ Notebooks/\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize \n",
    "\n",
    "from pathlib import Path \n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from mingpt.bpe import BPETokenizer \n",
    "from mingpt.utils import set_seed \n",
    "set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1858,   547,  1661,  ..., 42939,   757,    30])\n",
      "There were times—not many <SEP> but a few—when Jon Snow was glad he was a bastard <SEP> As he filled his wine cup once more from a passing flagon <SEP> it struck him that this might be one of them <SEP> He settled back in his place on the bench among the younger squires and drank <SEP> The sweet <SEP> fruity taste of summerwine filled his mouth and brought a smile to his lips <SEP> The Great Hall of Winterfell was hazy with smoke and heavy with the smell of roasted meat and fresh-baked bread <SEP> Its grey stone walls were draped with banners <SEP> White <SEP> gold <SEP> crimson: the direwolf of Stark <SEP> Baratheon ’ s crowned stag <SEP> the lion of Lannister <SEP> A singer was playing the high harp and reciting a ballad <SEP> but down at this end of the hall his voice could scarcely be heard above the roar of the fire <SEP> the clangor of pewter plates and cups <SEP> and the low mutter of a hundred drunken conversations <SEP> It was the fourth hour of the welcoming feast laid for the king <SEP> Jon ’ s brothers and sisters had been seated with the royal children <SEP> beneath the raised platform where Lord and Lady Stark hosted the king and queen <SEP> In honor of the occasion <SEP> his lord father would doubtless permit each child a glass of wine <SEP> but no more than that <SEP> Down here on the benches <SEP> there was no one to stop Jon drinking as much as he had a thirst for <SEP> And he was finding that he had a man ’ s thirst <SEP> to the raucous delight of the youths around him <SEP> who urged him on every time he drained a glass <SEP> They were fine company <SEP> and Jon relished the stories they were telling <SEP> tales of battle and bedding and the hunt <SEP> He was certain that his companions were more entertaining than the king ’ s offspring <SEP> He had sated his curiosity about the visitors when they made their entrance <SEP> The procession had passed not a foot from the place he had been given on the bench <SEP> and Jon had gotten a good long look at them all <SEP> His lord father had come first <SEP> escorting the queen <SEP> She was as beautiful as men said <SEP> A jeweled tiara gleamed amidst her long golden hair <SEP> its emeralds a perfect match for the green of her eyes <SEP> His father helped her up the steps to the dais and led her to her seat <SEP> but the queen never so much as looked at him <SEP> Even at fourteen <SEP> Jon could see through her smile <SEP> Next had come King Robert himself <SEP> with Lady Stark on his arm <SEP> The king was a great disappointment to Jon <SEP> His father had talked of him often: the peerless Robert Baratheon <SEP> demon of the Trident <SEP> the fiercest warrior of the realm <SEP> a giant among princes <SEP> Jon saw only a fat man <SEP> red-faced under his beard <SEP> sweating through his silks <SEP> He walked like a man half in his cups <SEP> After them came the children <SEP> Little Rickon first <SEP> managing the long walk with all the dignity a three-year-old could muster <SEP> Jon had to urge him on when he stopped to visit <SEP> Close behind came Robb <SEP> in grey wool trimmed with white <SEP> the Stark colors <SEP> He had the Princess Myrcella on his arm <SEP> She was a wisp of a girl <SEP> not quite eight <SEP> her hair a cascade of golden curls under a jeweled net <SEP> Jon noticed the shy looks she gave Robb as they passed between the tables and the timid way she smiled at him <SEP> He decided she was insipid <SEP> Robb didn ’ t even have the sense to realize how stupid she was; he was grinning like a fool <SEP> His half sisters escorted the royal princes <SEP> Arya was paired with plump young Tommen <SEP> whose white-blond hair was longer than hers <SEP> Sansa <SEP> two years older <SEP> drew the crown prince <SEP> Joffrey Baratheon <SEP> He was twelve <SEP> younger than Jon or Robb <SEP> but taller than either <SEP> to Jon ’ s vast dismay <SEP> Prince Joffrey had his sister ’ s hair and his mother ’ s deep green eyes <SEP> A thick tangle of blond curls dripped down past his golden choker and high velvet collar <SEP> Sansa looked radiant as she walked beside him <SEP> but Jon did not like Joffrey ’ s pouty lips or the bored <SEP> disdainful way he looked at Winterfell ’ s Great Hall <SEP> He was more interested in the pair that came behind him: the queen ’ s brothers <SEP> the Lannisters of Casterly Rock <SEP> The Lion and the Imp; there was no mistaking which was which <SEP> Ser Jaime Lannister was twin to Queen Cersei; tall and golden <SEP> with flashing green eyes and a smile that cut like a knife <SEP> He wore crimson silk <SEP> high black boots <SEP> a black satin cloak <SEP> On the breast of his tunic <SEP> the lion of his House was embroidered in gold thread <SEP> roaring its defiance <SEP> They called him the Lion of Lannister to his face and whispered  Kingslayer  behind his back <SEP> Jon found it hard to look away from him <SEP> This is what a king should look like <SEP> he thought to himself as the man passed <SEP> Then he saw the other one <SEP> waddling along half-hidden by his brother ’ s side <SEP> Tyrion Lannister <SEP> the youngest of Lord Tywin ’ s brood and by far the ugliest <SEP> All that the gods had given to Cersei and Jaime <SEP> they had denied Tyrion <SEP> He was a dwarf <SEP> half his brother ’ s height <SEP> struggling to keep pace on stunted legs <SEP> His head was too large for his body <SEP> with a brute ’ s squashed-in face beneath a swollen shelf of brow <SEP> One green eye and one black one peered out from under a lank fall of hair so blond it seemed white <SEP> Jon watched him with fascination <SEP> The last of the high lords to enter were his uncle <SEP> Benjen Stark of the Night ’ s Watch <SEP> and his father ’ s ward <SEP> young Theon Greyjoy <SEP> Benjen gave Jon a warm smile as he went by <SEP> Theon ignored him utterly <SEP> but there was nothing new in that <SEP> After all had been seated <SEP> toasts were made <SEP> thanks were given and returned <SEP> and then the feasting began <SEP> Jon had started drinking then <SEP> and he had not stopped <SEP> Something rubbed against his leg beneath the table <SEP> Jon saw red eyes staring up at him <SEP>  Hungry again?\n",
      "tensor([  258,  1965,  1279,  5188,    47,    29,  1318,   373,   991,  2063,\n",
      "          257, 12498,   276,  9015,   287,   262,  3641,   286,   262,  3084,\n",
      "         1279,  5188,    47,    29,  5966,  4251,   503,   284, 11626,   572,\n",
      "          257,  1232,  1279,  5188,    47,    29,   788,   550,   257,  1365,\n",
      "         2126,  1279,  5188,    47,    29,   679,   638,   361,   276,   262,\n",
      "         6512,  2187,   290,  1309,   262, 36756,   562, 10649,   284,   262,\n",
      "         4314,  1022,   465,  7405,  1279,  5188,    47,    29,  9897, 19551,\n",
      "          656,   340,   287, 27303,  9550,  1279,  5188,    47,    29,  2399,\n",
      "         9397,   290, 15153,   550,   407,   587, 10431,   284,  2222,   511,\n",
      "        23214,   284,   262, 47600,  1279,  5188,    47,    29,   475,   612,\n",
      "          547,   517, 13882,   621,  5966,   714,   954,   379,   428,   886,\n",
      "          286,   262,  6899,  1279,  5188,    47,    29,   290,   645,   530,\n",
      "          550,   531,   257,  1573,   546,   465, 15552,  1279,  5188,    47,\n",
      "           29,   679,  1297,  2241,   339,   373, 20200,   287,   326,  1165,\n",
      "         1279,  5188,    47,    29,  2399,  2951,   336,  2150,  1279,  5188,\n",
      "           47,    29,  5966, 31862,   379,   606, 27303,   306,  1279,  5188,\n",
      "           47,    29, 49766,   262,  7523,  1279,  5188,    47,    29,   679,\n",
      "        27961,  1194,   308, 29528,   286,  8237,   290,  7342,   465, 19958,\n",
      "        18829,  1614,   454,   262,  9015,  1279,  5188,    47,    29, 21367,\n",
      "         3888,  1022,   262,  8893,  1279,  5188,    47,    29, 25462,   706,\n",
      "          262,  7351,  4813,  1279,  5188,    47,    29,  1881,   286,   606,\n",
      "         1279,  5188,    47,    29,   257,  2042,   285,   506,  2411, 21551,\n",
      "          351,   890,  7872,  2951,  1279,  5188,    47,    29,  4978,   257,\n",
      "        21212,   286,   262,  9015,  1279,  5188,    47,    29,  1375,  5025,\n",
      "          290, 45871,   739,   262,  7624,   284,   651,   257,  2648,  1279,\n",
      "         5188,    47,    29,  5966,  7342,   262, 19207,  1279,  5188,    47,\n",
      "           29,   383, 21551,  1663,   992,  1877,   287,   607, 13589,   290,\n",
      "         3888,  5699,  1279,  5188,    47,    29,  9897,  3114,   510,  1279,\n",
      "         5188,    47,    29, 10574,  1279,  5188,    47,    29,   290,  5969,\n",
      "          262,  3290,   351,   883,  3024,  2266,  2951,  1279,  5188,    47,\n",
      "           29,   383, 21551, 20821,   281,  7954,  4427,  1279,  5188,    47,\n",
      "           29,  1375,   373,  1115,  1661,   262,  2546,   286,   262, 19958,\n",
      "        18829, 15552,  1279,  5188,    47,    29,  9897,   750,   407,  1445,\n",
      "         1279,  5188,    47,    29,   679,  6204,   625,   465, 11596,   290,\n",
      "         4721,   465,  5422,  1279,  5188,    47,    29,   275,  1723,   465,\n",
      "          277, 27725,  1279,  5188,    47,    29,   383, 21551, 11192,   276,\n",
      "         1279,  5188,    47,    29, 21405,   276,   757,  1279,  5188,    47,\n",
      "           29,   788,  1807,  1365,   286,   428,  1907,  1279,  5188,    47,\n",
      "           29,  1375,  2900,   290,  1017,  2954,  1497,  1279,  5188,    47,\n",
      "           29,   351,   530,   938, 36219, 11495,   284,  3613,   607, 11293,\n",
      "         1279,  5188,    47,    29,  9897,  1816,   736,   284,   465,  9799,\n",
      "         1279,  5188,    47,    29,  5966, 36268,   290,  4251,   739,   262,\n",
      "         3084,   284,   374, 18137,   262,   427,   363,  1360,  2330,  9230,\n",
      "         1279,  5188,    47,    29,   383, 19958, 18829,  3114,   510,   379,\n",
      "          683,  1279,  5188,    47,    29,   299,  3949, 15165,   379,   465,\n",
      "         1021,  1279,  5188,    47,    29,   788,  1816,   736,   284,  6600,\n",
      "         1279,  5188,    47,    29,  1148,   428,   530,   286,   262, 19958,\n",
      "        29664,   314,   564,   247,  1569,  2982,   523,   881,   286,    30])\n",
      "he asked <SEP> There was still half a honeyed chicken in the center of the table <SEP> Jon reached out to tear off a leg <SEP> then had a better idea <SEP> He knifed the bird whole and let the carcass slide to the floor between his legs <SEP> Ghost ripped into it in savage silence <SEP> His brothers and sisters had not been permitted to bring their wolves to the banquet <SEP> but there were more curs than Jon could count at this end of the hall <SEP> and no one had said a word about his pup <SEP> He told himself he was fortunate in that too <SEP> His eyes stung <SEP> Jon rubbed at them savagely <SEP> cursing the smoke <SEP> He swallowed another gulp of wine and watched his direwolf devour the chicken <SEP> Dogs moved between the tables <SEP> trailing after the serving girls <SEP> One of them <SEP> a black mongrel bitch with long yellow eyes <SEP> caught a scent of the chicken <SEP> She stopped and edged under the bench to get a share <SEP> Jon watched the confrontation <SEP> The bitch growled low in her throat and moved closer <SEP> Ghost looked up <SEP> silent <SEP> and fixed the dog with those hot red eyes <SEP> The bitch snapped an angry challenge <SEP> She was three times the size of the direwolf pup <SEP> Ghost did not move <SEP> He stood over his prize and opened his mouth <SEP> baring his fangs <SEP> The bitch tensed <SEP> barked again <SEP> then thought better of this fight <SEP> She turned and slunk away <SEP> with one last defiant snap to save her pride <SEP> Ghost went back to his meal <SEP> Jon grinned and reached under the table to ruffle the shaggy white fur <SEP> The direwolf looked up at him <SEP> nipped gently at his hand <SEP> then went back to eating <SEP> Is this one of the direwolves I ’ ve heard so much of?\n",
      "tensor([   64,  5385,  3809,  1965,  1969,   379,  1021,  1279,  5188,    47,\n",
      "           29,  5966,  3114,   510, 18177,   355,   465,  7711,  3932,  1234,\n",
      "          257,  1021,   319,   465,  1182,   290,   374,  1648,   992,   465,\n",
      "         4190,   881,   355,  5966,   550,   374,  1648,   992,   262, 17481,\n",
      "          564,   247,   264,  1279,  5188,    47,    29,   220,  3363,  1279,\n",
      "         5188,    47,    29,   220,   339,   531,  1279,  5188,    47,    29,\n",
      "          220,  2399,  1438,   318,  9897,  1279,  5188,    47,    29,  1881,\n",
      "          286,   262,  2809,  2387, 19072,   262,   275,   707,  9892,  1621,\n",
      "          339,   564,   247,   288,   587,  5149,   284,   787,  2119,   379,\n",
      "          262,  3084,   329,   511, 15876,   564,   247,   264,  3956,  1279,\n",
      "         5188,    47,    29,  3932, 48796, 20956,   965,  2860,   992,   262,\n",
      "         7624,   351,   890,  7405,   290,  1718,   262,  8237,  6508,   503,\n",
      "          286,  5966,   564,   247,   264,  1021,  1279,  5188,    47,    29,\n",
      "          220, 10216, 39002,  1279,  5188,    47,    29,   220,   339,   531,\n",
      "          706,   257,  6938,  1279,  5188,    47,    29,   220, 10528,   523,\n",
      "         6029,  1279,  5188,    47,    29,  1374,   867, 14180,   423,   345,\n",
      "          550,  1279,  5188,    47,    29,  5966,    30])\n",
      "a familiar voice asked close at hand <SEP> Jon looked up happily as his uncle Ben put a hand on his head and ruffled his hair much as Jon had ruffled the wolf ’ s <SEP>  Yes <SEP>  he said <SEP>  His name is Ghost <SEP> One of the squires interrupted the bawdy story he ’ d been telling to make room at the table for their lord ’ s brother <SEP> Benjen Stark straddled the bench with long legs and took the wine cup out of Jon ’ s hand <SEP>  Summerwine <SEP>  he said after a taste <SEP>  Nothing so sweet <SEP> How many cups have you had <SEP> Jon?\n",
      "tensor([18219, 13541,  1279,  5188,    47,    29,  3932, 20956, 13818,  1279,\n",
      "         5188,    47,    29,   220,  1081,   314, 15240,  1279,  5188,    47,\n",
      "           29,  7900,  1279,  5188,    47,    29,   880,  1279,  5188,    47,\n",
      "           29,   314,  1975,   314,   373,  7099,   621,   345,   262,   717,\n",
      "          640,   314,  1392,  4988,   290, 29093, 10785,    27,  5188,    47,\n",
      "           29,   220,   679,  3013, 14655,   257, 30490, 21670,  1279,  5188,\n",
      "           47,    29, 37472,  7586,   351, 43779,  1279,  5188,    47,    29,\n",
      "          422,   257,  6716,   256,   918,  2044,   290,  1643,   656,   340,\n",
      "         1279,  5188,    47,    29,   632,  1067,   403,  1740,  1279,  5188,\n",
      "           47,    29,  2399,  7711,   373,  7786,    12,    69, 20980,   290,\n",
      "          308, 12968,   355,   257,  8598,  1067,   363,  1279,  5188,    47,\n",
      "           29,   475,   612,   373,  1464,   257,  9254,   286, 20263,   287,\n",
      "          465,  4171,    12, 49502,  2951,  1279,  5188,    47,    29,   679,\n",
      "        12049,   287,  2042,  1279,  5188,    47,    29,   355,   307, 38631,\n",
      "          257,   582,   286,   262,  5265,   564,   247,   264,  6305,  1279,\n",
      "         5188,    47,    29, 27117,   340,   373,  5527,  2042, 47750,  1279,\n",
      "         5188,    47,    29,   351,  1029, 11620, 14412,   290,   257,  3094,\n",
      "        10999,   351,   257,  8465, 44983,  1279,  5188,    47,    29,   317,\n",
      "         4334,  8465,  6333,   373,  9052,   276,  2835,   465,  7393,  1279,\n",
      "         5188,    47,    29,  3932, 48796,  7342,  9897,   351, 30184,   355,\n",
      "          339, 15063,   465, 21670,  1279,  5188,    47,    29,   220,   317,\n",
      "          845,  5897, 17481,  1279,  5188,    47,    29,   220,   339,  6515,\n",
      "         1279,  5188,    47,    29,  5966,   531,  1279,  5188,    47,    29,\n",
      "         1279,    33,  2640,    29,   679,   564,   247,   264,   407,   588,\n",
      "          262,  1854,  1279,  5188,    47,    29,   679,  1239,  1838,   257,\n",
      "         2128,  1279,  5188,    47,    29,  1320,   564,   247,   264,  1521,\n",
      "          314,  3706,   683,  9897,  1279,  5188,    47,    29,  1320,  1279,\n",
      "         5188,    47,    29,   290,   780,   339,   564,   247,   264,  2330,\n",
      "         1279,  5188,    47,    29,   383,  1854,   389,   477,  3223,  1279,\n",
      "         5188,    47,    29, 13791,   393,  2042,  1279,  5188,    47,    29,\n",
      "         1279,    36,  2640,    29,  1318,   389,   991, 19958, 29664,  3675,\n",
      "          262,  5007,  1279,  5188,    47,    29,   775,  3285,   606,   319,\n",
      "          674, 28077,   654,    27,  5188,    47,    29,   220,  3932, 48796,\n",
      "        20956,  2921,  5966,   257,   890,   804,  1279,  5188,    47,    29,\n",
      "          220,  2094,   564,   247,   256,   345,  3221,  4483,   379,  3084,\n",
      "          351,   534,  9397,    30])\n",
      "Jon smiled <SEP> Ben Stark laughed <SEP>  As I feared <SEP> Ah <SEP> well <SEP> I believe I was younger than you the first time I got truly and sincerely drunk<SEP>  He snagged a roasted onion <SEP> dripping brown with gravy <SEP> from a nearby trencher and bit into it <SEP> It crunched <SEP> His uncle was sharp-featured and gaunt as a mountain crag <SEP> but there was always a hint of laughter in his blue-grey eyes <SEP> He dressed in black <SEP> as befitted a man of the Night ’ s Watch <SEP> Tonight it was rich black velvet <SEP> with high leather boots and a wide belt with a silver buckle <SEP> A heavy silver chain was looped round his neck <SEP> Benjen watched Ghost with amusement as he ate his onion <SEP>  A very quiet wolf <SEP>  he observed <SEP> Jon said <SEP> <BOS> He ’ s not like the others <SEP> He never makes a sound <SEP> That ’ s why I named him Ghost <SEP> That <SEP> and because he ’ s white <SEP> The others are all dark <SEP> grey or black <SEP> <EOS> There are still direwolves beyond the Wall <SEP> We hear them on our rangings<SEP>  Benjen Stark gave Jon a long look <SEP>  Don ’ t you usually eat at table with your brothers?\n",
      "tensor([18219,  9373,   287,   257,  6228,  3809,  1279,  5188,    47,    29,\n",
      "         1279,    33,  2640,    29,  4042,  1661,  1279,  5188,    47,    29,\n",
      "          887,  9975, 11182, 20956,  1807,   340,  1244,  1577, 13277,   284,\n",
      "          262, 15100,  1641,   284,  5852,   257, 31030,  1871,   606,  1279,\n",
      "         5188,    47,    29,  1279,    36,  2640,    29,   314,   766,    27,\n",
      "         5188,    47,    29,   220,  2399,  7711, 27846,   625,   465,  8163,\n",
      "          379,   262,  4376,  3084,   379,   262,  1290,   886,   286,   262,\n",
      "         6899,  1279,  5188,    47,    29,   220,  2011,  3956,   857,   407,\n",
      "         1283,   845, 43856,  9975,  1279,  5188,    47,    29,  5966,   550,\n",
      "         6810,   326,  1165,  1279,  5188,    47,    29,   317, 31030,   550,\n",
      "          284,  2193,   284,  4003,  1243,  1279,  5188,    47,    29,   284,\n",
      "         1100,   262,  3872,   326,   661, 24519,  2157,   511,  2951,  1279,\n",
      "         5188,    47,    29,  2399,  2988,   373, 21769,   477,   262,  2184,\n",
      "          274,   444,  1279,  5188,    47,    29,   475,   612,   373,  5381,\n",
      "         1108,   287,   683,   326,  5966,   550, 25129,  1775,   878,  1279,\n",
      "         5188,    47,    29,   679,   531,  1310,  1279,  5188,    47,    29,\n",
      "         2045,   503,   625,   262,  6899,   351, 14263,   276,  2951,  1279,\n",
      "         5188,    47,    29,  4379,  2147,  1279,  5188,    47,    29,  4930,\n",
      "         8632,  1497,  1279,  5188,    47,    29,   262,  5822,   550,   587,\n",
      "         7722,  7272,   477,  1755,  1279,  5188,    47,    29,  2399,  3154,\n",
      "         1986,   373, 44869,  2157,   465,  1049,  2042, 21213,  1279,  5188,\n",
      "           47,    29,   679,   925,   867,   257, 27805,  1279,  5188,    47,\n",
      "           29, 13818, 23112,   379,   790,   474,   395,  1279,  5188,    47,\n",
      "           29,   290,  7384,  1123,  9433,   588,   257, 34253,   582,  1279,\n",
      "         5188,    47,    29,   475, 13970,   683,   262, 16599,  3947,   355,\n",
      "         4692,   355,   281,  4771, 26924,  1279,  5188,    47,    29,  5966,\n",
      "         1297,   465,  7711,   287,   257,  1877,  1279,  5188,    47,    29,\n",
      "         5897,  3809,  1279,  5188,    47,    29,  1279,    33,  2640,    29,\n",
      "          383, 16599,   318,  7954,  1165,  1279,  5188,    47,    29,  9190,\n",
      "         1718,   262,  5822,   866,   284,   262,  8194,    82,   428,  6672,\n",
      "         1279,  5188,    47,    29,   383, 16599,  1422,   564,   247,   256,\n",
      "          765,   683,   284,   467,  1279,  5188,    47,    29,  1279,    36,\n",
      "         2640,    29,  3932, 48796,  2921,  5966,   257,  8161,  1279,  5188,\n",
      "           47,    29, 15964,   804,  1279,  5188,    47,    29,   220,   921,\n",
      "          836,   564,   247,   256,  2051,   881,  1279,  5188,    47,    29,\n",
      "          466,   345,  1279,  5188,    47,    29,  5966,    30])\n",
      "Jon answered in a flat voice <SEP> <BOS> Most times <SEP> But tonight Lady Stark thought it might give insult to the royal family to seat a bastard among them <SEP> <EOS> I see<SEP>  His uncle glanced over his shoulder at the raised table at the far end of the hall <SEP>  My brother does not seem very festive tonight <SEP> Jon had noticed that too <SEP> A bastard had to learn to notice things <SEP> to read the truth that people hid behind their eyes <SEP> His father was observing all the courtesies <SEP> but there was tightness in him that Jon had seldom seen before <SEP> He said little <SEP> looking out over the hall with hooded eyes <SEP> seeing nothing <SEP> Two seats away <SEP> the king had been drinking heavily all night <SEP> His broad face was flushed behind his great black beard <SEP> He made many a toast <SEP> laughed loudly at every jest <SEP> and attacked each dish like a starving man <SEP> but beside him the queen seemed as cold as an ice sculpture <SEP> Jon told his uncle in a low <SEP> quiet voice <SEP> <BOS> The queen is angry too <SEP> Father took the king down to the crypts this afternoon <SEP> The queen didn ’ t want him to go <SEP> <EOS> Benjen gave Jon a careful <SEP> measuring look <SEP>  You don ’ t miss much <SEP> do you <SEP> Jon?\n",
      "tensor([ 1135,   714,   779,   257,   582,   588,   345,   319,   262,  5007,\n",
      "         1279,  5188,    47,    29,  5966,  1509, 11978,   351, 11293,  1279,\n",
      "         5188,    47,    29,  1279,    33,  2640,    29, 31384,   318,   257,\n",
      "         7387,   300,   590,   621,   314,   716,  1279,  5188,    47,    29,\n",
      "          475,   314,   564,   247,   285,   262,  1365,  8429,  1279,  5188,\n",
      "           47,    29,   290, 28238,   268,  1139,   314,  1650,   257,  8223,\n",
      "          355,   880,   355,  2687,   287,   262, 16669,  1279,  5188,    47,\n",
      "           29,  1279,    36,  2640,    29,  1892,   540, 16970,  1279,  5188,\n",
      "           47,    29,  5966,   531,   287,   257,  4802, 10484,  1279,  5188,\n",
      "           47,    29,  1279,    33,  2640,    29,  7214,   502,   351,   345,\n",
      "          618,   345,   467,   736,   284,   262,  5007,  1279,  5188,    47,\n",
      "           29,  9190,   481,  1577,   502,  2666,   284,   467,   611,   345,\n",
      "         1265,   683,  1279,  5188,    47,    29,   314,   760,   339,   481,\n",
      "         1279,  5188,    47,    29,  1279,    36,  2640,    29, 23169,  3932,\n",
      "        48796,  9713,   465,  1986,  7773,  1279,  5188,    47,    29,   220,\n",
      "          383,  5007,   318,   257,  1327,  1295,   329,   257,  2933,  1279,\n",
      "         5188,    47,    29,  5966,  1279,  5188,    47,    29,  5966, 27278,\n",
      "         1279,  5188,    47,    29,  1279,    33,  2640,    29,   314,   716,\n",
      "         2048,   257,   582,  7334,  1279,  5188,    47,    29,   314,   481,\n",
      "         1210, 17280,   319,   616,  1306,  1438,  1110,  1279,  5188,    47,\n",
      "           29,   290,  6669,  7834,  6026,  5404,  1139, 19918,  1371,  1663,\n",
      "          510,  5443,   621,   584,  1751,  1279,  5188,    47,    29,  1279,\n",
      "           36,  2640,    29,  1320,   564,   247,   264,  2081,  1576,  1279,\n",
      "         5188,    47,    29,   220,  3932, 48796,   531,   351,   257, 20841,\n",
      "        14528,   286,   465,  5422,  1279,  5188,    47,    29,   679,  1718,\n",
      "         5966,   564,   247,   264,  6508,   422,   262,  3084,  1279,  5188,\n",
      "           47,    29,  5901,   340,  4713,   422,   257,  6716, 18086,  1279,\n",
      "         5188,    47,    29,   290, 24070,   866,   257,   890, 26633,  1279,\n",
      "         5188,    47,    29,  5966,   531,  1279,  5188,    47,    29,   383,\n",
      "         6960,  2851,   373,   530,   286,   465, 10281,  1279,  5188,    47,\n",
      "           29,  1279,    33,  2640,    29,  9637, 14226, 31089,   560,   268,\n",
      "          373,   691, 29167,   618,   339, 29346,   360,  8553,  1279,  5188,\n",
      "           47,    29,  1279,    36,  2640,    29,   317, 29179,   326, 15436,\n",
      "          257,  3931,  1279,  5188,    47,    29,   220,   465,  7711,  6235,\n",
      "          503,  1279,  5188,    47,    29,   220,  3406,  6387,  2677,  2626,\n",
      "         3478,  7319,  1450,  2263,   262,  1295,  1279,  5188,    47,    29,\n",
      "          290,  1194, 15334,  2111,   284,  1745,   340,  1279,  5188,    47,\n",
      "           29, 17877,   815,   423,  1297,   683,   326,  1175,  2125,   564,\n",
      "          247,   256,   257,   983,    27,  5188,    47,    29,   220,   679,\n",
      "         1718,  1194, 31145,   286,  8237,  1279,  5188,    47,    29,   220,\n",
      "         4418,  1279,  5188,    47,    29,   220,   339,   531,  1279,  5188,\n",
      "           47,    29, 36906,   465,  5422,  1279,  5188,    47,    29,   220,\n",
      "         9637, 14226, 31089,   560,   268,   373,   691, 29095,   618,   339,\n",
      "         3724,  1279,  5188,    47,    29,  1471,   423,   345, 11564,   326,\n",
      "          636,    30])\n",
      "We could use a man like you on the Wall <SEP> Jon swelled with pride <SEP> <BOS> Robb is a stronger lance than I am <SEP> but I ’ m the better sword <SEP> and Hullen says I sit a horse as well as anyone in the castle <SEP> <EOS> Notable achievements <SEP> Jon said in a sudden rush <SEP> <BOS> Take me with you when you go back to the Wall <SEP> Father will give me leave to go if you ask him <SEP> I know he will <SEP> <EOS> Uncle Benjen studied his face carefully <SEP>  The Wall is a hard place for a boy <SEP> Jon <SEP> Jon protested <SEP> <BOS> I am almost a man grown <SEP> I will turn fifteen on my next name day <SEP> and Maester Luwin says bastards grow up faster than other children <SEP> <EOS> That ’ s true enough <SEP>  Benjen said with a downward twist of his mouth <SEP> He took Jon ’ s cup from the table <SEP> filled it fresh from a nearby pitcher <SEP> and drank down a long swallow <SEP> Jon said <SEP> The Young Dragon was one of his heroes <SEP> <BOS> Daeren Targaryen was only fourteen when he conquered Dorne <SEP> <EOS> A conquest that lasted a summer <SEP>  his uncle pointed out <SEP>  Your Boy King lost ten thousand men taking the place <SEP> and another fifty trying to hold it <SEP> Someone should have told him that war isn ’ t a game<SEP>  He took another sip of wine <SEP>  Also <SEP>  he said <SEP> wiping his mouth <SEP>  Daeren Targaryen was only eighteen when he died <SEP> Or have you forgotten that part?\n",
      "tensor([18219, 32758,  1279,  5188,    47,    29,   383,  8237,   373,  1642,\n",
      "          683, 10758,  1279,  5188,    47,    29,   679,  3088,   284,  1650,\n",
      "          845,  3892,  1279,  5188,    47,    29,   284,   787,  2241,  1283,\n",
      "        25242,  1279,  5188,    47,    29,  1279,    33,  2640,    29,   314,\n",
      "         6044,  2147,  1279,  5188,    47,    29,   314,   765,   284,  4691,\n",
      "          287,   262,  5265,   564,   247,   264,  6305,  1279,  5188,    47,\n",
      "           29, 23169,  1279,  5188,    47,    29,  1279,    36,  2640,    29,\n",
      "          679,   550,  1807,   319,   340,   890,   290,  1327,  1279,  5188,\n",
      "           47,    29,  9105,   450,   276,   379,  1755,   981,   465,  9397,\n",
      "        21256,  1088,   683,  1279,  5188,    47,    29, 31384,   561, 25580,\n",
      "        16955, 10633, 23299,  1279,  5188,    47,    29,   561,  3141,  1049,\n",
      "        18837,   355,   262, 38498,   286,   262,  2258,  1279,  5188,    47,\n",
      "           29, 33628,   290,  8759,   261,   561,   307, 31384,   564,   247,\n",
      "          264, 17625,  3653,   290,  3896,  1745,    69,  5773,   287,   465,\n",
      "         1438,  1279,  5188,    47,    29,  2399, 15153, 39477,    64,   290,\n",
      "        20845,    64,   561, 12479,   262, 40862,   286,   584,  1049,  7777,\n",
      "          290,   467,  5366,   355, 37769,   286, 49203,   286,   511,   898,\n",
      "         1279,  5188,    47,    29,   887,   644,  1295,   714,   257, 31030,\n",
      "         2911,   284,  5160,    30])\n",
      "Jon boasted <SEP> The wine was making him bold <SEP> He tried to sit very straight <SEP> to make himself seem taller <SEP> <BOS> I forget nothing <SEP> I want to serve in the Night ’ s Watch <SEP> Uncle <SEP> <EOS> He had thought on it long and hard <SEP> lying abed at night while his brothers slept around him <SEP> Robb would someday inherit Winterfell <SEP> would command great armies as the Warden of the North <SEP> Bran and Rickon would be Robb ’ s bannermen and rule holdfasts in his name <SEP> His sisters Arya and Sansa would marry the heirs of other great houses and go south as mistress of castles of their own <SEP> But what place could a bastard hope to earn?\n",
      "tensor([ 1639,   836,   564,   247,   256,   760,   644,   345,   564,   247,\n",
      "          302,  4737,  1279,  5188,    47,    29,  5966,  1279,  5188,    47,\n",
      "           29,   383,  5265,   564,   247,   264,  6305,   318,   257, 20990,\n",
      "         3956,  2894,  1279,  5188,    47,    29,   775,   423,   645,  4172,\n",
      "         1279,  5188,    47,    29,  6045,   286,   514,   481,  1683,  2988,\n",
      "        11989,  1279,  5188,    47,    29,  3954,  3656,   318,  7077,  1279,\n",
      "         5188,    47,    29,  3954, 37769,   318,  7522,  1279,  5188,    47,\n",
      "           29,  5966,   531,  1279,  5188,    47,    29,  1279,    33,  2640,\n",
      "           29,   317, 31030,   460,   423,  7522,  1165,  1279,  5188,    47,\n",
      "           29,   314,   716,  3492,   284, 21192,   534, 17865,  1279,  5188,\n",
      "           47,    29,  1279,    36,  2640,    29,   921,   389,   257,  2933,\n",
      "          286, 29167,  1279,  5188,    47,    29,   220,  3932, 48796,   531,\n",
      "         1279,  5188,    47,    29,   220,  1892,   257,   582,  1279,  5188,\n",
      "           47,    29,   407,  1865,  1279,  5188,    47,    29, 14303,   345,\n",
      "          423,  1900,   257,  2415,  1279,  5188,    47,    29,   345,  2314,\n",
      "         1833,   644,   345,   561,   307,  3501,   510,  1279,  5188,    47,\n",
      "           29,  5966,   531, 50180,  1279,  5188,    47,    29,  1279,    33,\n",
      "         2640,    29,   314,   836,   564,   247,   256,  1337,   546,   326,\n",
      "            0])\n",
      "You don ’ t know what you ’ re asking <SEP> Jon <SEP> The Night ’ s Watch is a sworn brotherhood <SEP> We have no families <SEP> None of us will ever father sons <SEP> Our wife is duty <SEP> Our mistress is honor <SEP> Jon said <SEP> <BOS> A bastard can have honor too <SEP> I am ready to swear your oath <SEP> <EOS> You are a boy of fourteen <SEP>  Benjen said <SEP>  Not a man <SEP> not yet <SEP> Until you have known a woman <SEP> you cannot understand what you would be giving up <SEP> Jon said hotly <SEP> <BOS> I don ’ t care about that!\n",
      "tensor([   27,    36,  2640,    29,   921,  1244,  1279,  5188,    47,    29,\n",
      "          611,   345,  2993,   644,   340,  4001,  1279,  5188,    47,    29,\n",
      "          220,  3932, 48796,   531,  1279,  5188,    47,    29,   220,  1002,\n",
      "          345,  2993,   644,   262, 17865,   561,  1575,   345,  1279,  5188,\n",
      "           47,    29,   345,  1244,   307,  1342, 11069,   284,  1414,   262,\n",
      "         2756,  1279,  5188,    47,    29,  3367,  1279,  5188,    47,    29,\n",
      "         5966,  2936,  8993,  4485,  2641,   683,  1279,  5188,    47,    29,\n",
      "         1279,    33,  2640,    29,   314,   564,   247,   285,   407,   534,\n",
      "         3367,     0])\n",
      "<EOS> You might <SEP> if you knew what it meant <SEP>  Benjen said <SEP>  If you knew what the oath would cost you <SEP> you might be less eager to pay the price <SEP> son <SEP> Jon felt anger rise inside him <SEP> <BOS> I ’ m not your son!\n",
      "tensor([   27,    36,  2640,    29,  3932, 48796, 20956,  6204,   510,  1279,\n",
      "         5188,    47,    29,   220,  3125,   564,   247,   264,   262, 26246,\n",
      "           27,  5188,    47,    29,   220,   679,  1234,   257,  1021,   319,\n",
      "         5966,   564,   247,   264,  8163,  1279,  5188,    47,    29,   220,\n",
      "         7911,   736,   284,   502,   706,   345,   564,   247,  1569,  3735,\n",
      "         6083,   257,  1178, 19918,  1371,   286,   534,   898,  1279,  5188,\n",
      "           47,    29,   290,   356,   564,   247, 32660,   766,   703,   345,\n",
      "         1254,  1279,  5188,    47,    29,  5966, 29868,   992,  1279,  5188,\n",
      "           47,    29,   220,   314,   481,  1239,  2988,   257, 31030,  1279,\n",
      "         5188,    47,    29,   220,   339,   531,  7773,  1279,  5188,    47,\n",
      "           29,   220,  7236,     0])\n",
      "<EOS> Benjen Stark stood up <SEP>  More ’ s the pity<SEP>  He put a hand on Jon ’ s shoulder <SEP>  Come back to me after you ’ ve fathered a few bastards of your own <SEP> and we ’ ll see how you feel <SEP> Jon trembled <SEP>  I will never father a bastard <SEP>  he said carefully <SEP>  Never!\n"
     ]
    }
   ],
   "source": [
    "base_path = \"./\"\n",
    "fn = {\"small\": \"SmallSimpleCorpus.txt\", \"large\": \"LargerCorpus.txt\", \"got\": \"./data/Jon_1.txt\"}\n",
    "ds_choice = \"got\"\n",
    "truncation = -1  # int. If -1, then\n",
    "text = Path(base_path, fn[ds_choice]).read_text()\n",
    "if ds_choice == \"got\":\n",
    "    # Remove the newline char in the middle of sentences\n",
    "    # The \"paragraph splitting\" newlines appear to be \\n\\n -- remove the duplications there\n",
    "    text = text.replace(\"\\n\\n\", \"$$^^$$\").replace(\"\\n\", \" \").replace(\"$$^^$$\", \"\\n\")\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Train / test split\n",
    "train, val = train_test_split(sentences, test_size=0.2, shuffle=False)\n",
    "raw_data = train \n",
    "\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = BPETokenizer()\n",
    "data = []  # List of 1-d pytorch tensor\n",
    "for sent in raw_data[:10]:\n",
    "    tokenized = tokenizer(sent).view(-1)  # pytorch tensor\n",
    "    print(tokenized)\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare the dataset to train the Language Model (LM)\n",
    "This implementation splits the sentences and so doesn't create training \n",
    "examples that cross sentences.\n",
    "\n",
    "This code is set so that it uses one of two possible datasets, which were also used in Assignment 1: \n",
    "SmallSimpleCorpus.txt or LargerCorpus.txt\n",
    "\n",
    "Arguments:\n",
    "            ds_choice: str. \"small\" or \"large\". (i.e. selects which of the two datasets)\n",
    "            split: str. \"train\" or \"test\".\n",
    "            truncation: int. If -1: no truncation on sentences. Otherwise: truncate to this specific length.\n",
    "\"\"\" \n",
    "\n",
    "class LanguageModelingDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ds_choice=\"large\", split=\"train\", truncation=-1):\n",
    "        \n",
    "        base_path = \"./\"\n",
    "        fn = fn = {\"small\": \"SmallSimpleCorpus.txt\", \"large\": \"LargerCorpus.txt\", \"got\": \"./data/Jon_1.txt\"}\n",
    "        self.ds_choice = ds_choice\n",
    "        self.truncation = truncation  # int. If -1, then\n",
    "        text = Path(base_path, fn[ds_choice]).read_text()\n",
    "        if ds_choice == \"got\":\n",
    "            # Remove the newline char in the middle of sentences\n",
    "            # The \"paragraph splitting\" newlines appear to be \\n\\n -- remove the duplications there\n",
    "            text = text.replace(\"\\n\\n\", \"$$^^$$\").replace(\"\\n\", \" \").replace(\"$$^^$$\", \"\\n\")\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        # Train / test split\n",
    "        train, val = train_test_split(sentences, test_size=0.2, shuffle=False)\n",
    "        if split == \"train\":\n",
    "            raw_data = train \n",
    "        else:\n",
    "            raw_data = val \n",
    "\n",
    "        # Tokenize\n",
    "        self.tokenizer = BPETokenizer()\n",
    "        self.data = []  # List of 1-d pytorch tensor\n",
    "        for sent in raw_data:\n",
    "            tokenized = self.tokenizer(sent).view(-1)  # pytorch tensor\n",
    "            if truncation >= 0:\n",
    "                self.data.append(tokenized[:truncation])\n",
    "            else:\n",
    "                self.data.append(tokenized)\n",
    "\n",
    "        # Count some items\n",
    "        self.max_sentence_length = np.max([len(d) for d in self.data])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        \"\"\"\n",
    "        We have to set this to the max vocab size (i.e., that decided by the BPE tokenizer), \n",
    "        but actually, only a small number of vocab is used, especially for the small text. \n",
    "        \"\"\"\n",
    "        return 50257\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        The output should be a tuple x and y, both as pytorch tensors.\n",
    "        Please refer to the `run()` method in the mingpt/trainer.py script for \n",
    "        how the x and y are going to be used.\n",
    "        \"\"\"\n",
    "        x = self.data[idx][:-1]\n",
    "        y = self.data[idx][1:]\n",
    "        return (x, y)\n",
    "\n",
    "    def get_block_size(self):\n",
    "        \"\"\"\n",
    "        block_size is the size at which lines are truncated to ensure they are equal-length.\n",
    "        \"\"\"\n",
    "        return self.max_sentence_length\n",
    "    \n",
    "# Instantiate the Training Dataset\n",
    "#train_dataset = LanguageModelingDataset(ds_choice=\"small\", split=\"train\")  # use this for the short corpus\n",
    "train_dataset = LanguageModelingDataset(ds_choice=\"got\", split=\"train\", truncation=512) #use this for long\n",
    "\n",
    "# Instantiate a Validation Dataset (this is only really needed for the fine-tune task, not the LM task)\n",
    "#val_dataset = LanguageModelingDataset(ds_choice=\"small\", split=\"validation\")\n",
    "val_dataset = LanguageModelingDataset(ds_choice=\"got\", split=\"validation\", truncation=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([  258,  1965,  1279,  5188,    47,    29,  1318,   373,   991,  2063,\n",
      "          257, 12498,   276,  9015,   287,   262,  3641,   286,   262,  3084,\n",
      "         1279,  5188,    47,    29,  5966,  4251,   503,   284, 11626,   572,\n",
      "          257,  1232,  1279,  5188,    47,    29,   788,   550,   257,  1365,\n",
      "         2126,  1279,  5188,    47,    29,   679,   638,   361,   276,   262,\n",
      "         6512,  2187,   290,  1309,   262, 36756,   562, 10649,   284,   262,\n",
      "         4314,  1022,   465,  7405,  1279,  5188,    47,    29,  9897, 19551,\n",
      "          656,   340,   287, 27303,  9550,  1279,  5188,    47,    29,  2399,\n",
      "         9397,   290, 15153,   550,   407,   587, 10431,   284,  2222,   511,\n",
      "        23214,   284,   262, 47600,  1279,  5188,    47,    29,   475,   612,\n",
      "          547,   517, 13882,   621,  5966,   714,   954,   379,   428,   886,\n",
      "          286,   262,  6899,  1279,  5188,    47,    29,   290,   645,   530,\n",
      "          550,   531,   257,  1573,   546,   465, 15552,  1279,  5188,    47,\n",
      "           29,   679,  1297,  2241,   339,   373, 20200,   287,   326,  1165,\n",
      "         1279,  5188,    47,    29,  2399,  2951,   336,  2150,  1279,  5188,\n",
      "           47,    29,  5966, 31862,   379,   606, 27303,   306,  1279,  5188,\n",
      "           47,    29, 49766,   262,  7523,  1279,  5188,    47,    29,   679,\n",
      "        27961,  1194,   308, 29528,   286,  8237,   290,  7342,   465, 19958,\n",
      "        18829,  1614,   454,   262,  9015,  1279,  5188,    47,    29, 21367,\n",
      "         3888,  1022,   262,  8893,  1279,  5188,    47,    29, 25462,   706,\n",
      "          262,  7351,  4813,  1279,  5188,    47,    29,  1881,   286,   606,\n",
      "         1279,  5188,    47,    29,   257,  2042,   285,   506,  2411, 21551,\n",
      "          351,   890,  7872,  2951,  1279,  5188,    47,    29,  4978,   257,\n",
      "        21212,   286,   262,  9015,  1279,  5188,    47,    29,  1375,  5025,\n",
      "          290, 45871,   739,   262,  7624,   284,   651,   257,  2648,  1279,\n",
      "         5188,    47,    29,  5966,  7342,   262, 19207,  1279,  5188,    47,\n",
      "           29,   383, 21551,  1663,   992,  1877,   287,   607, 13589,   290,\n",
      "         3888,  5699,  1279,  5188,    47,    29,  9897,  3114,   510,  1279,\n",
      "         5188,    47,    29, 10574,  1279,  5188,    47,    29,   290,  5969,\n",
      "          262,  3290,   351,   883,  3024,  2266,  2951,  1279,  5188,    47,\n",
      "           29,   383, 21551, 20821,   281,  7954,  4427,  1279,  5188,    47,\n",
      "           29,  1375,   373,  1115,  1661,   262,  2546,   286,   262, 19958,\n",
      "        18829, 15552,  1279,  5188,    47,    29,  9897,   750,   407,  1445,\n",
      "         1279,  5188,    47,    29,   679,  6204,   625,   465, 11596,   290,\n",
      "         4721,   465,  5422,  1279,  5188,    47,    29,   275,  1723,   465,\n",
      "          277, 27725,  1279,  5188,    47,    29,   383, 21551, 11192,   276,\n",
      "         1279,  5188,    47,    29, 21405,   276,   757,  1279,  5188,    47,\n",
      "           29,   788,  1807,  1365,   286,   428,  1907,  1279,  5188,    47,\n",
      "           29,  1375,  2900,   290,  1017,  2954,  1497,  1279,  5188,    47,\n",
      "           29,   351,   530,   938, 36219, 11495,   284,  3613,   607, 11293,\n",
      "         1279,  5188,    47,    29,  9897,  1816,   736,   284,   465,  9799,\n",
      "         1279,  5188,    47,    29,  5966, 36268,   290,  4251,   739,   262,\n",
      "         3084,   284,   374, 18137,   262,   427,   363,  1360,  2330,  9230,\n",
      "         1279,  5188,    47,    29,   383, 19958, 18829,  3114,   510,   379,\n",
      "          683,  1279,  5188,    47,    29,   299,  3949, 15165,   379,   465,\n",
      "         1021,  1279,  5188,    47,    29,   788,  1816,   736,   284,  6600,\n",
      "         1279,  5188,    47,    29,  1148,   428,   530,   286,   262, 19958,\n",
      "        29664,   314,   564,   247,  1569,  2982,   523,   881,   286]), tensor([ 1965,  1279,  5188,    47,    29,  1318,   373,   991,  2063,   257,\n",
      "        12498,   276,  9015,   287,   262,  3641,   286,   262,  3084,  1279,\n",
      "         5188,    47,    29,  5966,  4251,   503,   284, 11626,   572,   257,\n",
      "         1232,  1279,  5188,    47,    29,   788,   550,   257,  1365,  2126,\n",
      "         1279,  5188,    47,    29,   679,   638,   361,   276,   262,  6512,\n",
      "         2187,   290,  1309,   262, 36756,   562, 10649,   284,   262,  4314,\n",
      "         1022,   465,  7405,  1279,  5188,    47,    29,  9897, 19551,   656,\n",
      "          340,   287, 27303,  9550,  1279,  5188,    47,    29,  2399,  9397,\n",
      "          290, 15153,   550,   407,   587, 10431,   284,  2222,   511, 23214,\n",
      "          284,   262, 47600,  1279,  5188,    47,    29,   475,   612,   547,\n",
      "          517, 13882,   621,  5966,   714,   954,   379,   428,   886,   286,\n",
      "          262,  6899,  1279,  5188,    47,    29,   290,   645,   530,   550,\n",
      "          531,   257,  1573,   546,   465, 15552,  1279,  5188,    47,    29,\n",
      "          679,  1297,  2241,   339,   373, 20200,   287,   326,  1165,  1279,\n",
      "         5188,    47,    29,  2399,  2951,   336,  2150,  1279,  5188,    47,\n",
      "           29,  5966, 31862,   379,   606, 27303,   306,  1279,  5188,    47,\n",
      "           29, 49766,   262,  7523,  1279,  5188,    47,    29,   679, 27961,\n",
      "         1194,   308, 29528,   286,  8237,   290,  7342,   465, 19958, 18829,\n",
      "         1614,   454,   262,  9015,  1279,  5188,    47,    29, 21367,  3888,\n",
      "         1022,   262,  8893,  1279,  5188,    47,    29, 25462,   706,   262,\n",
      "         7351,  4813,  1279,  5188,    47,    29,  1881,   286,   606,  1279,\n",
      "         5188,    47,    29,   257,  2042,   285,   506,  2411, 21551,   351,\n",
      "          890,  7872,  2951,  1279,  5188,    47,    29,  4978,   257, 21212,\n",
      "          286,   262,  9015,  1279,  5188,    47,    29,  1375,  5025,   290,\n",
      "        45871,   739,   262,  7624,   284,   651,   257,  2648,  1279,  5188,\n",
      "           47,    29,  5966,  7342,   262, 19207,  1279,  5188,    47,    29,\n",
      "          383, 21551,  1663,   992,  1877,   287,   607, 13589,   290,  3888,\n",
      "         5699,  1279,  5188,    47,    29,  9897,  3114,   510,  1279,  5188,\n",
      "           47,    29, 10574,  1279,  5188,    47,    29,   290,  5969,   262,\n",
      "         3290,   351,   883,  3024,  2266,  2951,  1279,  5188,    47,    29,\n",
      "          383, 21551, 20821,   281,  7954,  4427,  1279,  5188,    47,    29,\n",
      "         1375,   373,  1115,  1661,   262,  2546,   286,   262, 19958, 18829,\n",
      "        15552,  1279,  5188,    47,    29,  9897,   750,   407,  1445,  1279,\n",
      "         5188,    47,    29,   679,  6204,   625,   465, 11596,   290,  4721,\n",
      "          465,  5422,  1279,  5188,    47,    29,   275,  1723,   465,   277,\n",
      "        27725,  1279,  5188,    47,    29,   383, 21551, 11192,   276,  1279,\n",
      "         5188,    47,    29, 21405,   276,   757,  1279,  5188,    47,    29,\n",
      "          788,  1807,  1365,   286,   428,  1907,  1279,  5188,    47,    29,\n",
      "         1375,  2900,   290,  1017,  2954,  1497,  1279,  5188,    47,    29,\n",
      "          351,   530,   938, 36219, 11495,   284,  3613,   607, 11293,  1279,\n",
      "         5188,    47,    29,  9897,  1816,   736,   284,   465,  9799,  1279,\n",
      "         5188,    47,    29,  5966, 36268,   290,  4251,   739,   262,  3084,\n",
      "          284,   374, 18137,   262,   427,   363,  1360,  2330,  9230,  1279,\n",
      "         5188,    47,    29,   383, 19958, 18829,  3114,   510,   379,   683,\n",
      "         1279,  5188,    47,    29,   299,  3949, 15165,   379,   465,  1021,\n",
      "         1279,  5188,    47,    29,   788,  1816,   736,   284,  6600,  1279,\n",
      "         5188,    47,    29,  1148,   428,   530,   286,   262, 19958, 29664,\n",
      "          314,   564,   247,  1569,  2982,   523,   881,   286,    30]))\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.get_vocab_size())\n",
    "print(train_dataset.get_block_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_collate_fn(batch, device):\n",
    "    x = [item[0] for item in batch]  # List (len B) of varying lengths\n",
    "    y = [item[1] for item in batch]  # List (len B) of the same lengths as x\n",
    "    maxlen = max([len(s) for s in x])\n",
    "\n",
    "    padded_x, padded_y = [], []\n",
    "    for sx, sy in zip(x, y):\n",
    "        padded_x.append(torch.cat([sx, torch.ones(maxlen - len(sx))]))\n",
    "        padded_y.append(torch.cat([sy, torch.ones(maxlen - len(sy))]))\n",
    "    return torch.stack(padded_x).long().to(device), torch.stack(padded_y).long().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4],\n",
       "        [1, 2, 3, 4]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([torch.tensor([1,2,3,4]),torch.tensor([1,2,3,4])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18219, 32758,  1279,  5188,    47,    29,   383,  8237,   373,  1642,\n",
      "          683, 10758,  1279,  5188,    47,    29,   679,  3088,   284,  1650,\n",
      "          845,  3892,  1279,  5188,    47,    29,   284,   787,  2241,  1283,\n",
      "        25242,  1279,  5188,    47,    29,  1279,    33,  2640,    29,   314,\n",
      "         6044,  2147,  1279,  5188,    47,    29,   314,   765,   284,  4691,\n",
      "          287,   262,  5265,   564,   247,   264,  6305,  1279,  5188,    47,\n",
      "           29, 23169,  1279,  5188,    47,    29,  1279,    36,  2640,    29,\n",
      "          679,   550,  1807,   319,   340,   890,   290,  1327,  1279,  5188,\n",
      "           47,    29,  9105,   450,   276,   379,  1755,   981,   465,  9397,\n",
      "        21256,  1088,   683,  1279,  5188,    47,    29, 31384,   561, 25580,\n",
      "        16955, 10633, 23299,  1279,  5188,    47,    29,   561,  3141,  1049,\n",
      "        18837,   355,   262, 38498,   286,   262,  2258,  1279,  5188,    47,\n",
      "           29, 33628,   290,  8759,   261,   561,   307, 31384,   564,   247,\n",
      "          264, 17625,  3653,   290,  3896,  1745,    69,  5773,   287,   465,\n",
      "         1438,  1279,  5188,    47,    29,  2399, 15153, 39477,    64,   290,\n",
      "        20845,    64,   561, 12479,   262, 40862,   286,   584,  1049,  7777,\n",
      "          290,   467,  5366,   355, 37769,   286, 49203,   286,   511,   898,\n",
      "         1279,  5188,    47,    29,   887,   644,  1295,   714,   257, 31030,\n",
      "         2911,   284,  5160]) tensor([32758,  1279,  5188,    47,    29,   383,  8237,   373,  1642,   683,\n",
      "        10758,  1279,  5188,    47,    29,   679,  3088,   284,  1650,   845,\n",
      "         3892,  1279,  5188,    47,    29,   284,   787,  2241,  1283, 25242,\n",
      "         1279,  5188,    47,    29,  1279,    33,  2640,    29,   314,  6044,\n",
      "         2147,  1279,  5188,    47,    29,   314,   765,   284,  4691,   287,\n",
      "          262,  5265,   564,   247,   264,  6305,  1279,  5188,    47,    29,\n",
      "        23169,  1279,  5188,    47,    29,  1279,    36,  2640,    29,   679,\n",
      "          550,  1807,   319,   340,   890,   290,  1327,  1279,  5188,    47,\n",
      "           29,  9105,   450,   276,   379,  1755,   981,   465,  9397, 21256,\n",
      "         1088,   683,  1279,  5188,    47,    29, 31384,   561, 25580, 16955,\n",
      "        10633, 23299,  1279,  5188,    47,    29,   561,  3141,  1049, 18837,\n",
      "          355,   262, 38498,   286,   262,  2258,  1279,  5188,    47,    29,\n",
      "        33628,   290,  8759,   261,   561,   307, 31384,   564,   247,   264,\n",
      "        17625,  3653,   290,  3896,  1745,    69,  5773,   287,   465,  1438,\n",
      "         1279,  5188,    47,    29,  2399, 15153, 39477,    64,   290, 20845,\n",
      "           64,   561, 12479,   262, 40862,   286,   584,  1049,  7777,   290,\n",
      "          467,  5366,   355, 37769,   286, 49203,   286,   511,   898,  1279,\n",
      "         5188,    47,    29,   887,   644,  1295,   714,   257, 31030,  2911,\n",
      "          284,  5160,    30])\n",
      "X:  Jon boasted <SEP> The wine was making him bold <SEP> He tried to sit very straight <SEP> to make himself seem taller <SEP> <BOS> I forget nothing <SEP> I want to serve in the Night ’ s Watch <SEP> Uncle <SEP> <EOS> He had thought on it long and hard <SEP> lying abed at night while his brothers slept around him <SEP> Robb would someday inherit Winterfell <SEP> would command great armies as the Warden of the North <SEP> Bran and Rickon would be Robb ’ s bannermen and rule holdfasts in his name <SEP> His sisters Arya and Sansa would marry the heirs of other great houses and go south as mistress of castles of their own <SEP> But what place could a bastard hope to earn\n",
      "Y:   boasted <SEP> The wine was making him bold <SEP> He tried to sit very straight <SEP> to make himself seem taller <SEP> <BOS> I forget nothing <SEP> I want to serve in the Night ’ s Watch <SEP> Uncle <SEP> <EOS> He had thought on it long and hard <SEP> lying abed at night while his brothers slept around him <SEP> Robb would someday inherit Winterfell <SEP> would command great armies as the Warden of the North <SEP> Bran and Rickon would be Robb ’ s bannermen and rule holdfasts in his name <SEP> His sisters Arya and Sansa would marry the heirs of other great houses and go south as mistress of castles of their own <SEP> But what place could a bastard hope to earn?\n"
     ]
    }
   ],
   "source": [
    "# Print out an example of the data - this is processed more once it reaches lm_collate_fn (above)\n",
    "x,y = train_dataset[6]\n",
    "print(x, y)\n",
    "print(\"X: \",train_dataset.tokenizer.decode(x))\n",
    "print(\"Y: \",train_dataset.tokenizer.decode(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 7.29M\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-micro'\n",
    "model_config.vocab_size = train_dataset.get_vocab_size()\n",
    "model_config.block_size = train_dataset.get_block_size()\n",
    "model_config.n_classification_class = 2\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cpu\n"
     ]
    }
   ],
   "source": [
    "# Create a Trainer object and set the core hyper-parameters\n",
    "from mingpt.trainer import Trainer\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 3e-4 # the model we're using is so small that we can go a bit faster\n",
    "train_config.max_iters = 1000  # For small corpus: 3000 iterations is plenty. For large corpus: 100000 iterations is needed\n",
    "train_config.num_workers = 0\n",
    "train_config.batch_size = 8    # For small corpus, batch size of 4 is fine.  For large corpus use 16\n",
    "trainer = Trainer(train_config, model, train_dataset, val_dataset, collate_fn=lm_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_dt 0.00ms; iter 0: train loss 11.07419\n",
      "iter_dt 3362.16ms; iter 100: train loss 3.29252\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hf/w9g9d7vj0019fcrwqsw5y5c40000gn/T/ipykernel_85474/1872003065.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Train!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/School/ECE1786/final_project/GOTalk/mingpt/trainer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;31m# backprop and update the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_norm_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ECE1786/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ECE1786/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This function is called at the end of every batch in training\n",
    "# and is used to report the amount of time per 100 batches, and the loss at that point\n",
    "\n",
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "# Train!\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(trainer.device)\n",
    "# store the saved model in a file, so can re-use later\n",
    "modelsavename= \"model_small.pt\"  # change the name here to save in a specific file (and restore below)\n",
    "with open(modelsavename, \"wb\") as f:\n",
    "    torch.save(trainer.model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jon said.” he said.” “I said.” “I”'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the trained language model to predict a sequence of words following a few words\n",
    "encoded_prompt = train_dataset.tokenizer(\"Jon said\").to(trainer.device)\n",
    "generated_sequence = trainer.model.generate(encoded_prompt, trainer.device, temperature=1.5, max_new_tokens=20)\n",
    "train_dataset.tokenizer.decode(generated_sequence[0])\n",
    "# print([round(prob.item(),4) for prob in probs_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------  ------------------  -----------------  ----------------  ----------------  ----------------  ----------------  -----------------  ----------------  ------------------\n",
      "(' can', 0.5617)   (' hold', 0.6249)   (' a', 0.5445)     (' dog', 0.5756)  ('.', 0.9982)     (' cat', 0.607)   ('.', 0.9972)     (' cat', 0.7194)   (' and', 0.5736)  (' dog', 0.9899)\n",
      "(' hold', 0.2944)  (' rub', 0.3688)    (' the', 0.452)    (' cat', 0.424)   (' and', 0.0011)  (' dog', 0.3889)  (' .', 0.0017)    (' dog', 0.2796)   ('.', 0.4235)     (' cat', 0.0077)\n",
      "(' rub', 0.1411)   (' can', 0.0059)    (' and', 0.0032)   (' the', 0.0001)  (' .', 0.0006)    (' a', 0.0013)    (' and', 0.001)   (' the', 0.0003)   (' a', 0.0012)    (' I', 0.0012)\n",
      "(' holds', 0.002)  (' cat', 0.0001)    (' dog', 0.0001)   (' a', 0.0001)    (' rub', 0.0)     (' the', 0.0012)  (' rub', 0.0001)  (' a', 0.0003)     (' the', 0.0011)  (' rub', 0.0007)\n",
      "(' and', 0.0004)   (' holds', 0.0001)  (' cat', 0.0001)   (' holds', 0.0)   (' cat', 0.0)     (' and', 0.0011)  (' holds', 0.0)   (' and', 0.0003)   (' can', 0.0002)  (' hold', 0.0002)\n",
      "(' cat', 0.0001)   (' and', 0.0001)    (' hold', 0.0001)  (' hold', 0.0)    (' hold', 0.0)    ('.', 0.0002)     (' cat', 0.0)     (' hold', 0.0001)  (' rub', 0.0001)  (' holds', 0.0001)\n",
      "-----------------  ------------------  -----------------  ----------------  ----------------  ----------------  ----------------  -----------------  ----------------  ------------------\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "rows = []\n",
    "for i in range(6):\n",
    "    row = []\n",
    "    for probs in probs_seq:\n",
    "        row.append((train_dataset.tokenizer.decode(torch.tensor([probs[1][i]])),round(probs[0][i].item(),4)))\n",
    "    rows.append(row)\n",
    "print(tabulate(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She rubs a dog and cat. cat. cat. dog\n",
      "[0.4242, 0.5053, 0.7471, 0.9993, 0.9997, 0.6742, 0.8804, 0.7203, 0.9986, 0.6166]\n"
     ]
    }
   ],
   "source": [
    "# Another example\n",
    "encoded_prompt = train_dataset.tokenizer(\"She rubs\").to(trainer.device)\n",
    "generated_sequence, probs_seq = trainer.model.generate(encoded_prompt, trainer.device, temperature=0.6, max_new_tokens=10)\n",
    "print(train_dataset.tokenizer.decode(generated_sequence[0]))\n",
    "print([round(prob.item(),4) for prob in probs_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She holds a cat and dog.. dog. cat.\n",
      "[0.4604, 0.5518, 0.6688, 0.9997, 0.9997, 0.934, 0.541, 0.9994, 0.7498, 0.983]\n"
     ]
    }
   ],
   "source": [
    "# my example\n",
    "encoded_prompt = train_dataset.tokenizer(\"She holds\").to(trainer.device)\n",
    "generated_sequence, probs_seq = trainer.model.generate(encoded_prompt, trainer.device, temperature=0.6, max_new_tokens=10)\n",
    "print(train_dataset.tokenizer.decode(generated_sequence[0]))\n",
    "print([round(prob.item(),4) for prob in probs_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([  40, 1745,  290, 6437,  262, 3290]), tensor([1745,  290, 6437,  262, 3290,   13]))\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code below shows how to reload the model from the saved file; is useful things that take long to train\n",
    "model_large = \"model_large100K.pt\"\n",
    "model.load_state_dict(torch.load(model_large))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is a native\n",
      "of the New Jersey and the son\n"
     ]
    }
   ],
   "source": [
    "# Example showing how the reloaded model still works\n",
    "set_seed(99)\n",
    "encoded_prompt = train_dataset.tokenizer(\"He is\")\n",
    "generated_sequence, probs_seq = model.generate(encoded_prompt, device=\"cpu\", temperature=0.8, max_new_tokens=10)\n",
    "print(train_dataset.tokenizer.decode(generated_sequence[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ontario lake are in 1873. and shield as an were\n"
     ]
    }
   ],
   "source": [
    "set_seed(99)\n",
    "encoded_prompt = train_dataset.tokenizer(\"Ontario lake\")\n",
    "generated_sequence, probs_seq = model.generate(encoded_prompt, device=\"cpu\", temperature=0.8, max_new_tokens=10)\n",
    "print(train_dataset.tokenizer.decode(generated_sequence[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.5537e-10, 1.2851e-02, 1.2568e-09,  ..., 1.8367e-09, 1.3405e-09,\n",
      "         9.3107e-10]])\n",
      "tensor([[8.9296e-10, 5.7133e-03, 7.9823e-10,  ..., 1.0679e-09, 1.1928e-09,\n",
      "         8.4614e-10]])\n",
      "tensor([[9.4527e-11, 7.1178e-03, 1.0264e-10,  ..., 1.4708e-10, 1.1174e-10,\n",
      "         9.1374e-11]])\n",
      "tensor([[3.2113e-10, 3.2172e-02, 2.7883e-10,  ..., 4.4908e-10, 3.9936e-10,\n",
      "         3.0540e-10]])\n",
      "tensor([[2.9912e-10, 1.3020e-02, 2.6233e-10,  ..., 4.1762e-10, 2.9399e-10,\n",
      "         2.9330e-10]])\n",
      "tensor([[8.2578e-10, 1.2202e-02, 6.8875e-10,  ..., 9.8177e-10, 1.1165e-09,\n",
      "         7.2898e-10]])\n",
      "tensor([[3.3163e-10, 7.3499e-03, 2.6987e-10,  ..., 4.4147e-10, 3.5781e-10,\n",
      "         3.0072e-10]])\n",
      "tensor([[1.0116e-09, 4.2700e-02, 8.3285e-10,  ..., 1.2209e-09, 1.3048e-09,\n",
      "         1.0034e-09]])\n",
      "tensor([[9.4714e-10, 5.7228e-03, 6.5951e-10,  ..., 1.1590e-09, 9.6619e-10,\n",
      "         8.0748e-10]])\n",
      "tensor([[1.0553e-09, 8.7323e-03, 8.3025e-10,  ..., 1.1847e-09, 1.3750e-09,\n",
      "         9.1425e-10]])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hf/w9g9d7vj0019fcrwqsw5y5c40000gn/T/ipykernel_16311/3956893950.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mencoded_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"She rubs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgenerated_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_sequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/School/ECE1786/a3/mingpt/bpe.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m# ensure a simple 1D tensor for now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;31m# decode indices to text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datasets \n",
    "datasets.load_dataset(\"glue\", \"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6ac98cdf959e0cfc9a2a87b1d0a754a60151d74d55f7e6c3aa8003f1c5863c82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
