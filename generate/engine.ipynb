{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# model_name = \"huangtuoyue/GPT2-GOT2-finetuned\"\n",
    "model_name = \"huangtuoyue/GPT2-large-GOTfinetuned_v3\"\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model_name, tokenizer=model_name,\\\n",
    "    do_sample=True, max_length=100, temperature=0.9, top_p = 0.9, pad_token_id = 50256, repetition_penalty=1)\n",
    "\n",
    "output = generator(\"Once upon a time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Winter is coming.   A cold wind blew off the trees, and the wind picked up a few drops on Jon\\'s boots. He turned and ran, racing back toward the barracks. The wind had stopped, and darkness had spread across the yard. \" You\\'re my prisoner, \" Ser Alliser Thorne snarled. \" Tell them that, boy. \" Jon froze. \" Aye, they called me ‘prisoner,\\' \" he said quickly, \"but'}]\n"
     ]
    }
   ],
   "source": [
    "output = generator(\"Winter is coming. \")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello test'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TreebankWordDetokenizer().detokenize([\"hello\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winter is coming. Jon said \n",
      "invalid \n",
      "\n",
      "invalid \n",
      "\n",
      "invalid \n",
      "\n",
      "invalid \n",
      "\n",
      "invalid \n",
      "\n",
      "============= select dialogue ============\n",
      "A:The others are not so much. The Wall has men on it whose mothers were whores.; \n",
      "B:My lord, Lady Stark is waiting for me. She ’ s not going to let me leave Winterfell, not for a day or two. So I ’ ll stay a day and a night with him. Have you seen the Wall?; \n",
      "C: Yes, my lord. The Stark wordsmiths have done a man a kindness. They've carved his father ’ s brow into the likeness of a direwolf,\n",
      "Yes, my lord. The Stark wordsmiths have done a man a kindness. They've carved his father ’ s brow into the likeness of a direwolf,\n",
      "winter is coming. Jon said Yes, my lord. The Stark wordsmiths have done a man a kindness. They've carved his father ’ s brow into the likeness of a direwolf, he said. It ’ s the same as the one Jon saw on the kingsroad, only bigger.The next morning Jon heard Samwell Tarly declare that the middle name was bellied. He had ridden past the end of the world; he ’ d seen the Wall, and he\n",
      "context continue\n",
      "\n",
      "of a direwolf, he said. It ’ s the same as the one Jon saw on the kingsroad, only bigger.The next morning Jon heard Samwell Tarly declare that the middle name was bellied. He had ridden past the end of the world; he ’ d seen the Wall, and he ’ d sworn that he could feel it pressing down on him, and the air was colder than the breath of the First Men. When he told Eddard that the\n",
      "context continue\n",
      "\n",
      "name was bellied. He had ridden past the end of the world; he ’ d seen the Wall, and he ’ d sworn that he could feel it pressing down on him, and the air was colder than the breath of the First Men. When he told Eddard that the Wall was at his side, he did not sound as though he were breathing air. The air was colder than any knife he had ever felt. When he told him that he was at the Wall,\n",
      "context continue\n",
      "\n",
      "colder than the breath of the First Men. When he told Eddard that the Wall was at his side, he did not sound as though he were breathing air. The air was colder than any knife he had ever felt. When he told him that he was at the Wall, he sounded as though he were going to cry again.Jon urged them on as they walked, but Jon knew that he was not.When he and Sam climbed the tower steps, Jon ’ s tone was\n",
      "context continue\n",
      "\n",
      "knife he had ever felt. When he told him that he was at the Wall, he sounded as though he were going to cry again.Jon urged them on as they walked, but Jon knew that he was not.When he and Sam climbed the tower steps, Jon ’ s tone was flat and emotionless. He wanted to say, but he knew that what he would say, if he were to come straight from the stable, would be the last he would ever give to an authority.\n",
      "context continue\n",
      "\n",
      "he was not.When he and Sam climbed the tower steps, Jon ’ s tone was flat and emotionless. He wanted to say, but he knew that what he would say, if he were to come straight from the stable, would be the last he would ever give to an authority.His voice was high and emotionless, and Jon hated it. He wanted to spit on the ground, hate the people in it, hate the gods, but he couldn ’ t. He was\n",
      "context continue\n",
      "\n",
      "he were to come straight from the stable, would be the last he would ever give to an authority.His voice was high and emotionless, and Jon hated it. He wanted to spit on the ground, hate the people in it, hate the gods, but he couldn ’ t. He was so glad he was a bastard.His hand found itself shaking. He could feel the great weight of all that blame settling on him. All the hard work had gone on inside him. He was\n",
      "context continue\n",
      "\n",
      "invalid \n",
      "\n",
      "invalid \n",
      "\n",
      "============= select dialogue ============\n",
      "A:Are you ashamed of me?; \n",
      "B:So how do you feel now?; \n",
      "C: So why are you always so frightened? Jon asked.It ’ s because I ’ m a bastard,\n",
      "Are you ashamed of me?\n",
      "invalid \n",
      "\n",
      "in it, hate the gods, but he couldn ’ t. He was so glad he was a bastard.His hand found itself shaking. He could feel the great weight of all that blame settling on him. All the hard work had gone on inside him. He wasAre you ashamed of me? he asked Rast. The master-at-arms shrugged. A bastard is a soldier ’ s squire.Jon felt a coldness pass right through him. He pressed his lips together\n",
      "context continue\n",
      "\n",
      "great weight of all that blame settling on him. All the hard work had gone on inside him. He wasAre you ashamed of me? he asked Rast. The master-at-arms shrugged. A bastard is a soldier ’ s squire.Jon felt a coldness pass right through him. He pressed his lips together and made himself look at the face. A face that matched his own. He dared not look away.The guilt came pressing down on him as he waited behind R\n",
      "context continue\n",
      "\n",
      "shrugged. A bastard is a soldier ’ s squire.Jon felt a coldness pass right through him. He pressed his lips together and made himself look at the face. A face that matched his own. He dared not look away.The guilt came pressing down on him as he waited behind Rast. Down there in the dark, he ’ d heard the scrape of a boot on a stone. Perhaps that was why Lord Eddard had shon and shaved his beard.\n",
      "context continue\n",
      "\n",
      "face that matched his own. He dared not look away.The guilt came pressing down on him as he waited behind Rast. Down there in the dark, he ’ d heard the scrape of a boot on a stone. Perhaps that was why Lord Eddard had shon and shaved his beard.He was giving Jon a long look. The thought made him sad. There was a traitor ’ s nest deep in the woods. Could they all be like Jon ’ s friends?\n",
      "context continue\n",
      "\n",
      "scrape of a boot on a stone. Perhaps that was why Lord Eddard had shon and shaved his beard.He was giving Jon a long look. The thought made him sad. There was a traitor ’ s nest deep in the woods. Could they all be like Jon ’ s friends? he asked. My friends, his voice cut through the gloom of the armory. He remembered how he and Robb had gone to the armory. How happy they were. With their\n",
      "context continue\n",
      "\n",
      "There was a traitor ’ s nest deep in the woods. Could they all be like Jon ’ s friends? he asked. My friends, his voice cut through the gloom of the armory. He remembered how he and Robb had gone to the armory. How happy they were. With their permission?The armory was quiet. Too quiet. For a moment Jon ’ s heart sunk.Then he heard the laughter, sharp and insipid. Because I ’ m a\n",
      "context continue\n",
      "\n",
      "cut through the gloom of the armory. He remembered how he and Robb had gone to the armory. How happy they were. With their permission?The armory was quiet. Too quiet. For a moment Jon ’ s heart sunk.Then he heard the laughter, sharp and insipid. Because I ’ m a lord, Jon thought, I ’ m a lord, and you ’ re a boy. What of it?He tried to sit very straight, to\n",
      "context continue\n",
      "\n",
      "their permission?The armory was quiet. Too quiet. For a moment Jon ’ s heart sunk.Then he heard the laughter, sharp and insipid. Because I ’ m a lord, Jon thought, I ’ m a lord, and you ’ re a boy. What of it?He tried to sit very straight, to make himself seem taller. But he was too scared to come stand by the door. He pushed it inward, fighting to keep it from closing\n",
      "context continue\n",
      "\n",
      "invalid \n",
      "\n",
      "invalid \n",
      "\n",
      "invalid \n",
      "\n",
      "invalid \n",
      "\n",
      "invalid \n",
      "\n",
      "invalid \n",
      "\n",
      "invalid \n",
      "\n",
      "============= select dialogue ============\n",
      "A:Let me stand,; \n",
      "B:Let me stand by the door, and I can ’ t. I ’ m afraid of what might happen.; \n",
      "C: It ’ s a lie,\n",
      "It ’ s a lie,\n"
     ]
    }
   ],
   "source": [
    "f = open(\"./context_v3.txt\",\"w\")\n",
    "incorrect_generation = 0\n",
    "total = 0\n",
    "context = \"winter is coming. Jon said \"\n",
    "print(context)\n",
    "f.write(context)\n",
    "ITERATION = 3\n",
    "BOS = \"[BOS]\"\n",
    "EOS = \"[EOS]\"\n",
    "i = 0\n",
    "options = []\n",
    "pred_dialogue = False\n",
    "while i < ITERATION:\n",
    "    prev_context_end = len(context)\n",
    "    context_list = context.split()\n",
    "    if len(context_list) < 50: # used last 50 words to generate new context\n",
    "        run_context = TreebankWordDetokenizer().detokenize(context_list)\n",
    "    else: \n",
    "        run_context = TreebankWordDetokenizer().detokenize(context_list[-50:])\n",
    "    if pred_dialogue:\n",
    "        run_context += \" [BOS]\"\n",
    "    pred = generator(run_context)[0][\"generated_text\"]\n",
    "    # print(pred)\n",
    "    total += 1\n",
    "    # check [BOS]\n",
    "    words = pred.split()\n",
    "    eos_list = []\n",
    "    bos_list = []\n",
    "    for j, word in enumerate(words): \n",
    "        if word == BOS: \n",
    "            bos_list.append(j)\n",
    "        elif word == EOS:\n",
    "            eos_list.append(j)\n",
    "    if len(eos_list) == len(bos_list) and len(eos_list) == 0 and not pred_dialogue:\n",
    "        # a new context \n",
    "        context = pred\n",
    "        print(context)\n",
    "        f.write(context)\n",
    "        print(\"context continue\\n\")\n",
    "    elif len(eos_list) != len(bos_list): \n",
    "        print(\"invalid \\n\")\n",
    "        #discard\n",
    "        incorrect_generation += 1\n",
    "    else:\n",
    "        # valid bos eos position\n",
    "        valid = True \n",
    "        for j, value in enumerate(bos_list): \n",
    "            if eos_list[j] < value: \n",
    "                valid = False\n",
    "            if j > 0 and value < eos_list[j-1]: \n",
    "                # bos bos eos\n",
    "                valid = False\n",
    "        if not valid: \n",
    "            print(\"invalid\\n\")\n",
    "            incorrect_generation += 1\n",
    "        else:\n",
    "            # valid option\n",
    "            option = TreebankWordDetokenizer().detokenize(words[bos_list[0]+1:eos_list[0]])\n",
    "            options.append(option)\n",
    "            pred_dialogue = True\n",
    "            if len(options) == 3: \n",
    "                print(\"============= select dialogue ============\")\n",
    "                f.write(\"============= select dialogue ============\")\n",
    "                print(\"A:{}; \\nB:{}; \\nC: {}\".format(options[0], options[1], options[2]))\n",
    "                d = {\"A\": options[0], \"B\": options[1], \"C\": options[2]}\n",
    "                f.write(str(d))\n",
    "                valid = False\n",
    "                while not valid:\n",
    "                    user_input = input()\n",
    "                    if user_input in [\"A\", \"B\", \"C\"]: \n",
    "                        valid = True \n",
    "                \n",
    "                print(d[user_input])\n",
    "                f.write(d[user_input])\n",
    "                context += (d[user_input])\n",
    "                options = []\n",
    "                i += 1\n",
    "                pred_dialogue = False\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 39\n"
     ]
    }
   ],
   "source": [
    "print(incorrect_generation, total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteriaList, StoppingCriteria\n",
    "# from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huangtuoyue/GPT2-large-GOTfinetuned_v5\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"huangtuoyue/GPT2-large-GOTfinetuned_v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for stop dialogue generation\n",
    "class KeywordsStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, keywords_ids:list):\n",
    "        self.keywords = keywords_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        if input_ids[0][-1] in self.keywords:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# for context generation #\n",
    "# 1. stop if we get [BOS] -- Jon talking\n",
    "stop_context_words = ['[BOS]']\n",
    "stop_context_ids = [tokenizer.encode(w)[0] for w in stop_context_words]\n",
    "stop_context_criteria = KeywordsStoppingCriteria(stop_context_ids)\n",
    "# 2. exclude EOS\n",
    "bad_context_words = ['[EOS]']\n",
    "bad_context_ids = tokenizer(bad_context_words, add_special_tokens=False).input_ids\n",
    "\n",
    "# for dialogue generation #\n",
    "# 1. stop if we get [EOS] -- Jon stops talking\n",
    "stop_dialogue_words = ['[EOS]']\n",
    "stop_dialogue_ids = [tokenizer.encode(w)[0] for w in stop_dialogue_words]\n",
    "stop_dialogue_criteria = KeywordsStoppingCriteria(stop_dialogue_ids)\n",
    "# 2. exclude BOS\n",
    "bad_dialogue_words = ['[BOS]', ' \"', ' Jon', ' said', ' he', ' Snow']\n",
    "bad_dialogue_ids = tokenizer(bad_dialogue_words, add_special_tokens=False).input_ids\n",
    "\n",
    "def generate(text, pred_dialogue):\n",
    "\n",
    "    encoded_input = tokenizer(text, return_tensors='pt').input_ids\n",
    "    \n",
    "    # for context generation\n",
    "    if not pred_dialogue: \n",
    "\n",
    "      outputs = model.generate(encoded_input.to(device), do_sample=True, min_length=60, max_new_tokens=150, pad_token_id = 50256,\n",
    "                              temperature=0.95, top_p = 1, repetition_penalty = 1.1,\n",
    "                              stopping_criteria=StoppingCriteriaList([stop_context_criteria]), bad_words_ids = bad_context_ids)\n",
    "    \n",
    "    # for dialogue generation\n",
    "    else:\n",
    "      outputs = model.generate(encoded_input.to(device), do_sample=True, min_length=3, max_new_tokens=50, pad_token_id = 50256, \n",
    "                              temperature=1.1, top_p = 1, repetition_penalty = 1, \n",
    "                              stopping_criteria=StoppingCriteriaList([stop_dialogue_criteria]), bad_words_ids = bad_dialogue_ids)\n",
    "    \n",
    "    # deceode outputs and keep speical_tokens\n",
    "    res = tokenizer.batch_decode(outputs, skip_speical_tokens=False)\n",
    "    return res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introductions #\n",
    "# text = \"Jon grinned and reached under the table to ruffle the shaggy white fur. Inside, one of the guards looked at him and said\"\n",
    "# text = 'Ygritte:\"You know nothing, Jon Snow.\" Jon said'\n",
    "# text = \"Ben clapped Jon on the shoulder.\"\n",
    "# text = 'Daenerys: \"Hello, Jon Snow. It is a pleasure to meet you.\" Jon'\n",
    "text  = \"Jon Snow had been travelling for weeks, speaking to people in the villages and towns he visited.[BOS]\"\n",
    "# res, outputs = generate(text, True)\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./play.txt\",\"w\")\n",
    "incorrect_generation = 0\n",
    "total = 0\n",
    "context = \"Jon Snow had been travelling for weeks, speaking to people in the villages and towns he visited.\"\n",
    "print(context)\n",
    "f.write(context)\n",
    "ITERATION = 5\n",
    "BOS = \"[BOS]\"\n",
    "EOS = \"[EOS]\"\n",
    "i = 0\n",
    "options = []\n",
    "pred_dialogue = False\n",
    "\n",
    "while i < ITERATION:\n",
    "    prev_context_end_idx = len(context)\n",
    "    context_list = context.split()\n",
    "\n",
    "    # check context length #\n",
    "    if len(context_list) < 50: # used last 50 words to generate new context\n",
    "        run_context = TreebankWordDetokenizer().detokenize(context_list)\n",
    "        last_idx = len(run_context)\n",
    "\n",
    "    else: \n",
    "        run_context = TreebankWordDetokenizer().detokenize(context_list[-50:])\n",
    "        last_idx = len(run_context)\n",
    "    \n",
    "    # dialogue generation #\n",
    "    if pred_dialogue:\n",
    "        # print(run_context)\n",
    "        pred = generate(run_context, pred_dialogue)\n",
    "    # context generation #\n",
    "    else:\n",
    "        # print(run_context)\n",
    "        pred = generate(run_context, pred_dialogue)\n",
    "    \n",
    "    # count total generation time #\n",
    "    total += 1\n",
    "\n",
    "    # check if generation includes [BOS] & [EOS] #\n",
    "    words = pred.split()\n",
    "    eos_list = []\n",
    "    bos_list = []\n",
    "    for j, word in enumerate(words): \n",
    "        if word == BOS: \n",
    "            bos_list.append(j)\n",
    "        elif word == EOS:\n",
    "            eos_list.append(j)\n",
    "    \n",
    "    # Validity for context generation #\n",
    "    if words[-1] == '[BOS]' and not pred_dialogue: # correct context\n",
    "    # if '[BOS]' in words: # correct context\n",
    "        context = pred\n",
    "        # print(context)\n",
    "        print(context[last_idx:-5] + \":\")\n",
    "        print(\"\\n\")\n",
    "        f.write(context[last_idx:-5]+ \":\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        pred_dialogue = True\n",
    "\n",
    "    elif words[-1] != '[BOS]' and not pred_dialogue: # no [BOS]\n",
    "        # print(\"Invalid Context: no BOS or too long \\n\")\n",
    "        incorrect_generation += 1\n",
    "\n",
    "        pred_dialogue = False\n",
    "\n",
    "    # Validity for dialogue generation #\n",
    "    elif pred_dialogue:\n",
    "        if words[-1] != '[EOS]':\n",
    "            # print(\"Invalid Dialogue: no EOS or too long \\n\")\n",
    "            incorrect_generation += 1\n",
    "\n",
    "        elif words[-1] == '[EOS]': # correct dialogue\n",
    "            option = TreebankWordDetokenizer().detokenize(words[bos_list[-1]+1:eos_list[-1]])\n",
    "            options.append(option)\n",
    "            pred_dialogue = True\n",
    "            if len(options) == 3: \n",
    "                print(\"\\n\")\n",
    "                print(\"============= Select Dialogue ============\")\n",
    "                print(\"A:{} \\nB:{} \\nC: {}\".format(options[0], options[1], options[2]))\n",
    "                d = {\"A\": options[0], \"B\": options[1], \"C\": options[2]}\n",
    "                valid = False\n",
    "\n",
    "                user_input = input()\n",
    "                # while not valid:\n",
    "                #     user_input = input()\n",
    "                #     if user_input in [\"A\", \"B\", \"C\"]: \n",
    "                #         valid = True\n",
    "                print(\"Choice Selected: \" + user_input)\n",
    "                print(\"============= Context Contined ============\")\n",
    "                print(\"\\n\")\n",
    "\n",
    "                f.write(\"\\n\")\n",
    "                f.write(\"============= Select Dialogue ============\")\n",
    "                f.write(\"\\n\")\n",
    "                f.write(\"A:{} \\nB:{} \\nC:{}\".format(options[0], options[1], options[2]))\n",
    "                f.write(\"\\n\\n\")\n",
    "                f.write(\"Choice Selected: \" + user_input + \"\\n\")\n",
    "                f.write(\"============= Context Contined ============\")\n",
    "                f.write(\"\\n\\n\")\n",
    "\n",
    "                context += (\" \" + d[user_input] + \" [EOS]\")\n",
    "                # print(context)\n",
    "                options = []\n",
    "                i += 1\n",
    "                pred_dialogue = False\n",
    "\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(incorrect_generation, total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d128fe176aead1b5f2c446f268fcdf10cba0d3cb275513d8767b97bd302f321f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
